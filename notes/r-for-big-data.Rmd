---
title: "R for Big Data"
author: "Marcel Ramos"
date: "October 17, 2015"
output: 
  beamer_presentation:
    theme: "CambridgeUS"
    colortheme: "dolphin"
    fonttheme: "structureitalicserif"
---

Using R for Big Data
==============================================
* Big Data 
* Tidy Data
* Handling large datasets
* Efficiency in data manipulation
* Pipes for fluid and readable programming

Big Data\footnote{\tiny{Slide adapted from Hadley Wickham}}
==============================================
Size   | Description
------ | -------
Big    | Can't fit in memory on one computer: >5 TB
Medium | Fits in memory on a server: 10 GB - 5 TB
Small  | Fits in memory on a laptop: <10 GB

\begin{block}{Note:}
R is great at small!
\end{block}

Principles of Tidy Data
==============================================
* Often said: 80% of data analysis is cleaning/munging
* Provide a standard way of organizing data\footnote{\tiny{http://vita.had.co.nz/papers/tidy-data.pdf}}
1. Each variable forms a column
2. Each observation forms a row
3. Each type of observational unit forms a table

Dataset     | Variable    | Variable
----------- | ----------- | -----------
Observation | Value       | Value
Observation | Value       | Value


Principles of Tidy Data (2)
==============================================


```{r, echo = TRUE}
library(nycflights13)
library(dplyr)
flights %>% group_by(carrier) %>% 
  summarise(avg_depdelay = mean(dep_delay, na.rm = TRUE)) %>% 
  merge(., airlines) %>% arrange(avg_depdelay)
```

```{r , echo = TRUE}
longdata <- gather(iris, key = measure, n, Sepal.Length:Petal.Width) %>% 
  separate(measure, c("type", "dimension"))
longdata %>% group_by(Species, type, dimension) %>% summarise(avg_dim = mean(n, na.rm = TRUE))
```
